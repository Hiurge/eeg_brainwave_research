{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project Description\n","\n","**What**: Create a machine/deep learning based model which will sucessfully predict if a 20 minute baseline EEG session is that of a schizophrenic.\n","\n","**Why**: To enhance quick diagnosis of patients that land in the emergency room to get to know if they're schizophrenic and need specializied care.\n","\n","### Notes:\n","* There are obvious issues that can arise from the contextual setting of landing on ER that will have to be accounted for if we manage to make a working model based on the dataset.\n","\n","---\n","\n","## Notebook Description\n","\n","This notebook is meant for exploration and notes for the [TUH EEG](https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/) dataset, alongside with ideas on what to extract from it.\n","\n","### Notes:\n","* The next step will be to extract the necessary samples. \n","    * around 300-500 of control and maybe 100 of diagnosed schizophrenics (not on meds) to enhance our initial dataset.\n","\n","### Goals:\n","* Find the best way to traverse through the [TUH EEG](https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/) dataset.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Structure\n","\n","* Based on the directory structure, we won't be able to (and there's no need) to download everything locally. \n","* Based on this [README.txt](https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/_AAREADME.txt), it looks like the .txt files inside of each patients data contains `the EEG report corresponding to the patient and session`, which is (based on some samples) written in semi-structured way. This enables keyword search the directory tree.\n","    * Not sure if blast of GET requests is a good idea. Need to maybe space them by 1 sec each just not to DDOS their system. Maybe even through a VPN just in case?"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset statistics\n","```\n","---\n"," Files and Sessions:\n","\n","               no. patients: 13,539\n","               no. sessions: 23,002\n","  avg. no. sessions/patient: 1.70\n","              no. edf files: 53,506\n","             total duration: 56,726,510 secs (15,757 hrs)\n","\n"," Signal Data:\n","   over 40 different channel configurations\n","   sample frequency varies from 250 Hz to 1024 Hz\n","   95% of the data includes a 10/20 configuration as a subset of the\n","      available channels\n","\n","---\n","```\n","* ~The only real issue I see is with the channel configurations... should we include or exclude for now anything with a different than our standard (btw, what IS our standard?)?~ \n","    * NVM, it looks like most of the dataset contains 10/20 configuration subset, which saves us quite nicely.\n","* Quite high sample frequency, in pre-processing we might look to downgrading it a bit maybe? \n","* Not sure about patients with multiple sessions. Might be that the best/closest score we can get for the additional schizophrenics will be the first session (hopefully they might contain non-drug EEG sessions).\n","    * And as for control, might be just best to use unique patients data."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"import requests\nimport requests.auth\nfrom requests.auth import HTTPBasicAuth\nimport re\nfrom collections import namedtuple\nfrom time import sleep\nimport pandas as pd\nfrom IPython.display import clear_output\n\n\ns = requests.Session()\ns.auth = (HTTPBasicAuth('some_auth', 'some_auth')) # Paste real auth here.\n\nEDFData = namedtuple(\"EDFData\", [\"url\", \"readme\"])\n\nclass MultipleFilesFound(Exception):\n    \"\"\"Raised when multiple files found when only one should be found.\"\"\"\n    pass\n\nclass NoFilesFound(Exception):\n    \"\"\"Raised when no files where found but we excepted something\"\"\"\n    pass\n\ndef crawl(init_link):\n    to_crawl = [init_link]\n    i = 0\n    while to_crawl:\n#         if i > 100:\n#             break\n        \n        current_url = to_crawl.pop(-1)\n        r = s.get(current_url)\n        # First link is always parent directory link\n        new_dirs = re.findall(r'<a href=\"([^?].*\\/)\">', r.text)[1:]\n        \n        to_crawl.extend([current_url + new_url for new_url in new_dirs])\n        \n        # If we get to the end, download the TXT content and save as tuple\n        try:\n            if not new_dirs:\n                txt_file_url = re.findall(r'<a href=\"([^?].*\\.txt)\">', r.text)\n                # A couple of check statements to get sure we only get one TXT file.\n                if not txt_file_url:\n                    raise NoFilesFound \n                if len(txt_file_url) > 2:\n                    raise MultipleFilesFound\n                i+=1\n                txt_file_content = s.get(current_url + txt_file_url[0])\n                clear_output(wait=True)\n                print(f\"Found {i} sessions out of 23,002. {(i/23002)*100:.2f}%\")\n                \n                yield EDFData(url=current_url, readme=txt_file_content.text)\n\n        except NoFilesFound:\n            with open(\"no_files_error.txt\", \"a+\") as f:\n                f.write(f\"{current_url}\\n\")\n        except MultipleFilesFound:\n            with open(\"multiple_files_error.txt\", \"a+\") as f:\n                f.write(f\"{current_url}\\n\")\n                \n        \nedf_reports = crawl(\"https://isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/edf/\")\n# Segmenting it so there are 5 parts. Last time it disconnected right by the end.\nedf_reports_df = pd.DataFrame().from_records(edf_reports, columns=EDFData._fields)\nedf_reports_df.to_csv(\"edf_reports_all.csv\", index=False)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Async Version\nBelow you can find the async version of the script above. It cuts down time from around 1 to 1,5 hours to 15 minutes.\n\nPlease run it outside of Jupyter as it has its own event loop.\n\nPlease note that the fetch_content could be done better, but I haven't had time to polish it."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import asyncio\nimport aiohttp\nimport re\nfrom collections import namedtuple\nimport pandas as pd\nimport time\n\nfetch_counter = 0\nEDFData = namedtuple(\"EDFData\", [\"url\", \"readme\"])\n\n\n# Quick error classes in case something happens\nclass MultipleFilesFound(Exception):\n    \"\"\"Raised when multiple files found when only one should be found.\"\"\"\n    pass\n\n\nclass NoFilesFound(Exception):\n    \"\"\"Raised when no files where found but we excepted something\"\"\"\n    pass\n\n\n# MAIN ASYNC SCRIPT\nasync def fetch_content(session, url):\n    try:\n        async with session.get(url) as response:\n            try:\n                return await response.text()\n            # In some cases it couldn't read it because\n            # it couldn't convert to UTF8 due to some artifacts.\n            # Needs a better exception handling, for now it works.\n            except Exception:\n                return await response.content.read()\n    # General exceptions are bad. Might be better to specify Timeout\n    # and to bind it to a max level of recursion.\n    # Had to add it because the sheer volume of requests probably\n    # triggered those timeouts.\n    except Exception as e:\n        print(\"Caught a timeout error...\")\n        await asyncio.sleep(30)\n        return await fetch_content(session, url)\n\n\nasync def get_relevant_content(session, url, relevant_data):\n    global fetch_counter\n\n    url_content = await asyncio.shield(fetch_content(session, url))\n    new_dirs = re.findall(r'<a href=\"([^?].*\\/)\">', url_content)[1:]\n    to_crawl = [url + new_url for new_url in new_dirs]\n\n    if to_crawl:\n        tasks = [get_relevant_content(session, new_url, relevant_data)\n                 for new_url in to_crawl]\n        for f in asyncio.as_completed(tasks):\n            data = await f\n            if data:\n                relevant_data.append(data)\n\n    if not to_crawl:\n        txt_file_url = re.findall(r'<a href=\"([^?].*\\.txt)\">', url_content)\n        # A couple of check statements to get sure we only get one TXT file.\n        if not txt_file_url:\n            raise NoFilesFound\n        if len(txt_file_url) > 2:\n            raise MultipleFilesFound\n        fetch_counter += 1\n        print(f\"Found {fetch_counter} sessions out of 23,002. {(fetch_counter/23002)*100:.2f}%\")\n        txt_file_content = await fetch_content(session, url + txt_file_url[0])\n\n        return EDFData(\n            url=url,\n            readme=txt_file_content\n        )\n\n\nasync def async_crawl(init_url):\n    \"\"\"Main function.\n\n    Note:\n        Works by adding the found data to a list which is\n        specified in this function.\n        Please update the BasicAuth with proper login info.\n    \"\"\"\n    data = []\n    # Add the auth info below\n    async with aiohttp.ClientSession(auth=aiohttp.BasicAuth(AUTH_INFO, AUTH_INFO), headers={\"Connection\": \"close\"}) as s:\n        await get_relevant_content(s, init_url, data)\n\n    return data\n\n# Might change this to directly dropping it in a DataFrame.\nt1 = time.time()\nrecords = asyncio.run(async_crawl(\"https://isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/edf/\"))\nt2 = time.time()\nprint(f\"Time it took: {t2-t1}\")\nprint(f\"Length: {len(records)} (make sure it's 23,002)\")\n\nedf_reports_df = pd.DataFrame().from_records(records, columns=EDFData._fields)\nedf_reports_df.to_csv(\"edf_reports_all.csv\")\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}