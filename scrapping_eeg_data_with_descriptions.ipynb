{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "**What**: Create a machine/deep learning based model which will sucessfully predict if a 20 minute baseline EEG session is that of a schizophrenic.\n",
    "\n",
    "**Why**: To enhance quick diagnosis of patients that land in the emergency room to get to know if they're schizophrenic and need specializied care.\n",
    "\n",
    "### Notes:\n",
    "* There are obvious issues that can arise from the contextual setting of landing on ER that will have to be accounted for if we manage to make a working model based on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Description\n",
    "\n",
    "This notebook is meant for exploration and notes for the [TUH EEG](https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/) dataset, alongside with ideas on what to extract from it.\n",
    "\n",
    "### Notes:\n",
    "* The next step will be to extract the necessary samples. \n",
    "    * around 300-500 of control and maybe 100 of diagnosed schizophrenics (not on meds) to enhance our initial dataset.\n",
    "\n",
    "### Goals:\n",
    "* Find the best way to traverse through the [TUH EEG](https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/) dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure\n",
    "\n",
    "* Based on the directory structure, we won't be able to (and there's no need) to download everything locally. \n",
    "* Based on this [README.txt](https://www.isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/_AAREADME.txt), it looks like the .txt files inside of each patients data contains `the EEG report corresponding to the patient and session`, which is (based on some samples) written in semi-structured way. This enables keyword search the directory tree.\n",
    "    * Not sure if blast of GET requests is a good idea. Need to maybe space them by 1 sec each just not to DDOS their system. Maybe even through a VPN just in case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset statistics\n",
    "```\n",
    "---\n",
    " Files and Sessions:\n",
    "\n",
    "               no. patients: 13,539\n",
    "               no. sessions: 23,002\n",
    "  avg. no. sessions/patient: 1.70\n",
    "              no. edf files: 53,506\n",
    "             total duration: 56,726,510 secs (15,757 hrs)\n",
    "\n",
    " Signal Data:\n",
    "   over 40 different channel configurations\n",
    "   sample frequency varies from 250 Hz to 1024 Hz\n",
    "   95% of the data includes a 10/20 configuration as a subset of the\n",
    "      available channels\n",
    "\n",
    "---\n",
    "```\n",
    "* ~The only real issue I see is with the channel configurations... should we include or exclude for now anything with a different than our standard (btw, what IS our standard?)?~ \n",
    "    * NVM, it looks like most of the dataset contains 10/20 configuration subset, which saves us quite nicely.\n",
    "* Quite high sample frequency, in pre-processing we might look to downgrading it a bit maybe? \n",
    "* Not sure about patients with multiple sessions. Might be that the best/closest score we can get for the additional schizophrenics will be the first session (hopefully they might contain non-drug EEG sessions).\n",
    "    * And as for control, might be just best to use unique patients data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "s = requests.Session()\n",
    "s.auth = (HTTPBasicAuth('some_auth', 'some_auth')) # Paste real auth here.\n",
    "\n",
    "EDFData = namedtuple(\"EDFData\", [\"url\", \"readme\"])\n",
    "\n",
    "class MultipleFilesFound(Exception):\n",
    "    \"\"\"Raised when multiple files found when only one should be found.\"\"\"\n",
    "    pass\n",
    "\n",
    "class NoFilesFound(Exception):\n",
    "    \"\"\"Raised when no files where found but we excepted something\"\"\"\n",
    "    pass\n",
    "\n",
    "def crawl(init_link):\n",
    "    to_crawl = [init_link]\n",
    "    i = 0\n",
    "    while to_crawl:\n",
    "#         if i > 100:\n",
    "#             break\n",
    "        \n",
    "        current_url = to_crawl.pop(-1)\n",
    "        r = s.get(current_url)\n",
    "        # First link is always parent directory link\n",
    "        new_dirs = re.findall(r'<a href=\"([^?].*\\/)\">', r.text)[1:]\n",
    "        \n",
    "        to_crawl.extend([current_url + new_url for new_url in new_dirs])\n",
    "        \n",
    "        # If we get to the end, download the TXT content and save as tuple\n",
    "        try:\n",
    "            if not new_dirs:\n",
    "                txt_file_url = re.findall(r'<a href=\"([^?].*\\.txt)\">', r.text)\n",
    "                # A couple of check statements to get sure we only get one TXT file.\n",
    "                if not txt_file_url:\n",
    "                    raise NoFilesFound \n",
    "                if len(txt_file_url) > 2:\n",
    "                    raise MultipleFilesFound\n",
    "                i+=1\n",
    "                txt_file_content = s.get(current_url + txt_file_url[0])\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Found {i} sessions out of 23,002. {(i/23002)*100:.2f}%\")\n",
    "                \n",
    "                yield EDFData(url=current_url, readme=txt_file_content.text)\n",
    "\n",
    "        except NoFilesFound:\n",
    "            with open(\"no_files_error.txt\", \"a+\") as f:\n",
    "                f.write(f\"{current_url}\\n\")\n",
    "        except MultipleFilesFound:\n",
    "            with open(\"multiple_files_error.txt\", \"a+\") as f:\n",
    "                f.write(f\"{current_url}\\n\")\n",
    "                \n",
    "        \n",
    "edf_reports = crawl(\"https://isip.piconepress.com/projects/tuh_eeg/downloads/tuh_eeg/v1.1.0/edf/\")\n",
    "# Segmenting it so there are 5 parts. Last time it disconnected right by the end.\n",
    "edf_reports_df = pd.DataFrame().from_records(edf_reports, columns=EDFData._fields)\n",
    "edf_reports_df.to_csv(\"edf_reports_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
